{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax\n",
    "\n",
    "Jax is package for \"composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more\".\n",
    "* Core features\n",
    "    * [Automatic differentiation](#Automatic-differentiation)\n",
    "    * [Vectorization with vmap](#Vectorization-with-jax.vmap)\n",
    "    * [Just-in-time compilation](#Just-in-time-compilation-with-jax.jit)\n",
    "* Submodules\n",
    "    * [Random-number-generation](#Random-number-generation)\n",
    "* Use cases\n",
    "    * [Gradient-based optimization](#Use-case:-Gradient-based-optimization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jax.grad produces a function handle for the derivative of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(0.07065082, dtype=float32)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(np.tanh)(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order derivatives can be simply chained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(-0.13621867, dtype=float32)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(jax.grad(np.tanh))(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword argument `argnums` specifies with respect to which arguments the function is to be differentiated.\n",
    "`argnums` can also take a tuple to calculated multiple gradients at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray(2., dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "dfoo_dx = jax.grad(foo, argnums=0)\n",
    "dfoo_dx(0.1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function value and gradient can be calculated simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(DeviceArray(3.2, dtype=float32), DeviceArray(2., dtype=float32))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_grad_foo = jax.value_and_grad(foo, argnums=0)\n",
    "val_grad_foo(0.1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian\n",
    "In order to calculate the gradient of vector-valued functions we need to use `jacfwd` or `jacrev` instead of `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1.         0.84147096 0.5403023 ]\n[1.         0.84147096 0.5403023 ]\n[1.         0.84147096 0.5403023 ]\n"
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"Scalar-valued function with 3 inputs.\"\"\"\n",
    "    x1, x2, x3 = x\n",
    "    return x1 + x2 * np.sin(x3)\n",
    "\n",
    "x = np.ones(3)\n",
    "print(jax.grad(f)(x))\n",
    "print(jax.jacrev(f)(x))\n",
    "print(jax.jacfwd(f)(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "f(x):\n [0.84147096 1.        ]\ndf/dx(x):\n[[0.84147096 0.5403023  0.        ]\n [1.         1.         2.        ]]\n"
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"Vector-valued function with 3 inputs and 2 outputs\"\"\"\n",
    "    x1, x2, x3 = x\n",
    "    return np.array([\n",
    "        x1 * np.sin(x2),\n",
    "        x1 * x2 * x3**2\n",
    "    ])\n",
    "\n",
    "x = np.ones(3)\n",
    "print('f(x):\\n', f(x))\n",
    "print('df/dx(x):')\n",
    "print(jax.jacfwd(f)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian\n",
    "For the Hessian we simply call the jacobian twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "H, shape (2, 3, 3)\n[[[ 0.          0.5403023   0.        ]\n  [ 0.5403023  -0.84147096  0.        ]\n  [ 0.          0.          0.        ]]\n\n [[ 0.          1.          2.        ]\n  [ 1.          0.          2.        ]\n  [ 2.          2.          2.        ]]]\n"
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jax.jacfwd(jax.jacrev(f))\n",
    "\n",
    "H = hessian(f)(x)\n",
    "print('H, shape', H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with jax.vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([[[0.84147096, 0.5403023 ],\n              [0.        , 0.        ],\n              [0.        , 0.        ]],\n\n             [[0.        , 0.        ],\n              [0.84147096, 0.5403023 ],\n              [0.        , 0.        ]],\n\n             [[0.        , 0.        ],\n              [0.        , 0.        ],\n              [0.84147096, 0.5403023 ]]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[:, 0] * np.sin(x[:, 1])\n",
    "\n",
    "x = np.ones((3, 2))\n",
    "jax.jacrev(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "f(x): [0.84147096 0.84147096 0.84147096 0.84147096 0.84147096]\n"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0] * np.sin(x[1])\n",
    "\n",
    "x = np.ones((5, 2))\n",
    "y = jax.vmap(f)(x)\n",
    "print('f(x):', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just-in-time compilation with jax.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.84147096 0.5403023 ]\n [0.84147096 0.5403023 ]\n [0.84147096 0.5403023 ]\n [0.84147096 0.5403023 ]\n [0.84147096 0.5403023 ]]\n"
    }
   ],
   "source": [
    "J = jax.vmap(jax.jacfwd(f))(x)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[[ 0.          0.5403023 ]\n  [ 0.5403023  -0.84147096]]\n\n [[ 0.          0.5403023 ]\n  [ 0.5403023  -0.84147096]]\n\n [[ 0.          0.5403023 ]\n  [ 0.5403023  -0.84147096]]\n\n [[ 0.          0.5403023 ]\n  [ 0.5403023  -0.84147096]]\n\n [[ 0.          0.5403023 ]\n  [ 0.5403023  -0.84147096]]]\n"
    }
   ],
   "source": [
    "H = jax.vmap(jax.jacfwd(jax.jacrev(f)))(x)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random number generation\n",
    "Jax has its own pseudo random number generation system that focusses on parallel usage.\n",
    "A pecularity compared to e.g. `numpy` is that calls to the random number generator don't modify the random key, thus multiple calls with the same key yield the same random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "key = [ 0 42] x = [0.57414436 0.10015821 0.05946112]\nkey = [ 0 42] x = [0.57414436 0.10015821 0.05946112]\n"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "for i in range(2):\n",
    "    x = jax.random.uniform(key, shape=(3,))\n",
    "    print('key =', key, 'x =', x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to explicitly create new keys by splitting an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "key = [  64467757 2916123636] x = [0.05988741 0.19778168 0.13219142]\nkey = [2350016172 1168365246] x = [0.853312   0.68688035 0.85908866]\n"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    key, = jax.random.split(key, num=1)  # num new keys are created\n",
    "    x = jax.random.uniform(key, shape=(3,))\n",
    "    print('key =', key, 'x =', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use case: Gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use jax to provide gradients to the SLSQP optimizer in `scipy.optimize` in order to M\n",
    "minimize the bivariate Rosenbrock, subject to some non-linear (in)equality constraints and box bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Optimization terminated successfully.    (Exit mode 0)\n            Current function value: 0.34271758794784546\n            Iterations: 4\n            Function evaluations: 5\n            Gradient evaluations: 4\n"
    },
    {
     "data": {
      "text/plain": "     fun: 0.34271758794784546\n     jac: array([-0.82676458, -0.41372478])\n message: 'Optimization terminated successfully.'\n    nfev: 5\n     nit: 4\n    njev: 4\n  status: 0\n success: True\n       x: array([0.41494475, 0.1701105 ])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def rosen(x):\n",
    "    return np.sum(100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2)\n",
    "\n",
    "def ineq(x):\n",
    "    return np.array([\n",
    "        1 - x[0] - 2 * x[1],\n",
    "        1 - x[0]**2 - x[1],\n",
    "        1 - x[0]**2 + x[1]\n",
    "    ])\n",
    "\n",
    "def eq(x):\n",
    "    return np.array([2 * x[0] + x[1] - 1])\n",
    "\n",
    "res = scipy.optimize.minimize(\n",
    "    rosen,\n",
    "    jac=jax.grad(rosen),\n",
    "    x0=np.array([0.5, 0]),\n",
    "    bounds=[(0, 1), (-0.5, 2.0)],\n",
    "    constraints=[\n",
    "        dict(type='eq', fun=eq, jac=jax.jacfwd(eq)),\n",
    "        dict(type='ineq', fun=ineq, jac=jax.jacfwd(ineq)) \n",
    "    ], \n",
    "    options={'ftol': 1e-9, 'disp': True},\n",
    "    method='SLSQP',\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}