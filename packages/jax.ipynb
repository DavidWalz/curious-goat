{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax\n",
    "\n",
    "Jax is package for \"composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more\".\n",
    "* Core features\n",
    "    * [Automatic differentiation](#Automatic-differentiation)\n",
    "    * [Vectorization with vmap](#Vectorization-with-jax.vmap)\n",
    "    * [Just-in-time compilation](#Just-in-time-compilation-with-jax.jit)\n",
    "    * [Gotchas](#Gotchas)\n",
    "* Submodules\n",
    "    * [Random-number-generation](#Random-number-generation)\n",
    "* Use cases\n",
    "    * [Gradient-based optimization](#Use-case:-Gradient-based-optimization)\n",
    "    * [Differentiable polynomial regression model](#Use-case:-Differentiable-polynomial-regression-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jax.grad produces a function handle for the derivative of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walzds/.local/lib/python3.8/site-packages/jax/lib/xla_bridge.py:119: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.07065082, dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(np.tanh)(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order derivatives can be simply chained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.13621867, dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(jax.grad(np.tanh))(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword argument `argnums` specifies with respect to which arguments the function is to be differentiated.\n",
    "`argnums` can also take a tuple to calculated multiple gradients at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "df_dx = jax.grad(f, argnums=0)  # df/dx\n",
    "df_dx(0.1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function value and gradient can be calculated simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(3.2, dtype=float32), DeviceArray(2., dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.value_and_grad(f, argnums=0)(0.1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives can be functions involving python control flow operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.2, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    if x < 1:\n",
    "        return x\n",
    "    else:\n",
    "        return x**2\n",
    "\n",
    "jax.grad(f)(1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian\n",
    "In order to calculate the gradient of vector-valued functions we need to use `jacfwd` or `jacrev` instead of `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.84147096 0.5403023 ]\n",
      "[1.         0.84147096 0.5403023 ]\n",
      "[1.         0.84147096 0.5403023 ]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"Scalar-valued function with 3 inputs.\"\"\"\n",
    "    x1, x2, x3 = x\n",
    "    return x1 + x2 * np.sin(x3)\n",
    "\n",
    "x = np.ones(3)\n",
    "print(jax.grad(f)(x))\n",
    "print(jax.jacrev(f)(x))\n",
    "print(jax.jacfwd(f)(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x):\n",
      " [0.84147096 1.        ]\n",
      "df/dx(x):\n",
      "[[0.84147096 0.5403023  0.        ]\n",
      " [1.         1.         2.        ]]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"Vector-valued function with 3 inputs and 2 outputs\"\"\"\n",
    "    x1, x2, x3 = x\n",
    "    return np.array([\n",
    "        x1 * np.sin(x2),\n",
    "        x1 * x2 * x3**2\n",
    "    ])\n",
    "\n",
    "x = np.ones(3)\n",
    "print('f(x):\\n', f(x))\n",
    "print('df/dx(x):')\n",
    "print(jax.jacfwd(f)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian\n",
    "For the Hessian we simply call the jacobian twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H, shape (2, 3, 3)\n",
      "[[[ 0.          0.5403023   0.        ]\n",
      "  [ 0.5403023  -0.84147096  0.        ]\n",
      "  [ 0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          1.          2.        ]\n",
      "  [ 1.          0.          2.        ]\n",
      "  [ 2.          2.          2.        ]]]\n"
     ]
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jax.jacfwd(jax.jacrev(f))\n",
    "\n",
    "H = hessian(f)(x)\n",
    "print('H, shape', H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with jax.vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.84147096, 0.5403023 ],\n",
       "              [0.        , 0.        ],\n",
       "              [0.        , 0.        ]],\n",
       "\n",
       "             [[0.        , 0.        ],\n",
       "              [0.84147096, 0.5403023 ],\n",
       "              [0.        , 0.        ]],\n",
       "\n",
       "             [[0.        , 0.        ],\n",
       "              [0.        , 0.        ],\n",
       "              [0.84147096, 0.5403023 ]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[:, 0] * np.sin(x[:, 1])\n",
    "\n",
    "x = np.ones((3, 2))\n",
    "jax.jacrev(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x): [0.84147096 0.84147096 0.84147096 0.84147096 0.84147096]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0] * np.sin(x[1])\n",
    "\n",
    "x = np.ones((5, 2))\n",
    "y = jax.vmap(f)(x)\n",
    "print('f(x):', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just-in-time compilation with jax.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84147096 0.5403023 ]\n",
      " [0.84147096 0.5403023 ]\n",
      " [0.84147096 0.5403023 ]\n",
      " [0.84147096 0.5403023 ]\n",
      " [0.84147096 0.5403023 ]]\n"
     ]
    }
   ],
   "source": [
    "J = jax.vmap(jax.jacfwd(f))(x)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.          0.5403023 ]\n",
      "  [ 0.5403023  -0.84147096]]\n",
      "\n",
      " [[ 0.          0.5403023 ]\n",
      "  [ 0.5403023  -0.84147096]]\n",
      "\n",
      " [[ 0.          0.5403023 ]\n",
      "  [ 0.5403023  -0.84147096]]\n",
      "\n",
      " [[ 0.          0.5403023 ]\n",
      "  [ 0.5403023  -0.84147096]]\n",
      "\n",
      " [[ 0.          0.5403023 ]\n",
      "  [ 0.5403023  -0.84147096]]]\n"
     ]
    }
   ],
   "source": [
    "H = jax.vmap(jax.jacfwd(jax.jacrev(f)))(x)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gotchas\n",
    "Index-based operations have to be modified using `index_add`, `index_update` and `index` so that autograd can handle them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.ops import index, index_add, index_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<class 'jax.interpreters.xla.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x = np.ones((3, 5))\n",
    "    x[::2, 3:] += 4\n",
    "except TypeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1., 1., 1., 5., 5.],\n",
       "             [1., 1., 1., 1., 1.],\n",
       "             [1., 1., 1., 5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((3, 5))\n",
    "x = index_add(x, index[::2, 3:], 4.)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[4., 4., 4., 4., 4.],\n",
       "             [1., 1., 1., 1., 1.],\n",
       "             [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((3, 5))\n",
    "x = index_update(x, index[0, :], 4.)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random number generation\n",
    "Jax has its own pseudo random number generation system that focusses on parallel usage.\n",
    "A pecularity compared to e.g. `numpy` is that calls to the random number generator don't modify the random key, thus multiple calls with the same key yield the same random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key = [ 0 42] x = [0.57414436 0.10015821 0.05946112]\n",
      "key = [ 0 42] x = [0.57414436 0.10015821 0.05946112]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "for i in range(2):\n",
    "    x = jax.random.uniform(key, shape=(3,))\n",
    "    print('key =', key, 'x =', x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly create new keys by splitting an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key = [  64467757 2916123636] x = [0.05988741 0.19778168 0.13219142]\n",
      "key = [2350016172 1168365246] x = [0.853312   0.68688035 0.85908866]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    key, = jax.random.split(key, num=1)  # num new keys are created\n",
    "    x = jax.random.uniform(key, shape=(3,))\n",
    "    print('key =', key, 'x =', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: Gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use jax to provide gradients to the SLSQP optimizer in `scipy.optimize` in order to M\n",
    "minimize the bivariate Rosenbrock, subject to some non-linear (in)equality constraints and box bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.34271758794784546\n",
      "            Iterations: 4\n",
      "            Function evaluations: 5\n",
      "            Gradient evaluations: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 0.34271758794784546\n",
       "     jac: array([-0.82676458, -0.41372478])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 5\n",
       "     nit: 4\n",
       "    njev: 4\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([0.41494475, 0.1701105 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def rosen(x):\n",
    "    return np.sum(100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2)\n",
    "\n",
    "def ineq(x):\n",
    "    return np.array([\n",
    "        1 - x[0] - 2 * x[1],\n",
    "        1 - x[0]**2 - x[1],\n",
    "        1 - x[0]**2 + x[1]\n",
    "    ])\n",
    "\n",
    "def eq(x):\n",
    "    return np.array([2 * x[0] + x[1] - 1])\n",
    "\n",
    "res = scipy.optimize.minimize(\n",
    "    rosen,\n",
    "    jac=jax.grad(rosen),\n",
    "    x0=np.array([0.5, 0]),\n",
    "    bounds=[(0, 1), (-0.5, 2.0)],\n",
    "    constraints=[\n",
    "        dict(type='eq', fun=eq, jac=jax.jacfwd(eq)),\n",
    "        dict(type='ineq', fun=ineq, jac=jax.jacfwd(ineq)) \n",
    "    ], \n",
    "    options={'ftol': 1e-9, 'disp': True},\n",
    "    method='SLSQP',\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: Differentiable polynomial regression model\n",
    "Sklearn models generally don't provide gradients $df(x)/dx$. Here we set up a multivariate polynomial regression model with jax, in which the fitted coefficients (linreg, PLS, ...) from sklearn can be plugged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy data, x in R^3 -> y in R^1\n",
    "onp.random.seed(42)\n",
    "N = 20\n",
    "X_data = onp.random.rand(N, 3)\n",
    "x1, x2, x3 = X_data.T\n",
    "Y_data = 1 + x1 + 0.2 * x1 * x2 + x1**2 - 0.2 * x2 * x3 - 0.1 * x3**2 + 0.1 * onp.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coefficients\n",
      "[ 0.98178781  0.84156272  0.53594876 -0.40367642  1.24280767 -0.25484535\n",
      "  0.78677898 -0.38437492 -0.17086278 -0.14827962]\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.pipeline.Pipeline([\n",
    "    (\"polyfeat\", sklearn.preprocessing.PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)),\n",
    "    (\"linreg\", sklearn.linear_model.LinearRegression(fit_intercept=False)),\n",
    "])\n",
    "model.fit(X_data, Y_data)\n",
    "p = model[\"linreg\"].coef_\n",
    "print(f\"model coefficients\\n{p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(n_features, degree, interaction_only, include_bias):\n",
    "    n_features = len(x)\n",
    "    comb = (it.combinations if interaction_only else it.combinations_with_replacement)\n",
    "    start = int(not include_bias)\n",
    "    return it.chain.from_iterable(comb(range(n_features), i) for i in range(start, degree + 1))\n",
    "\n",
    "def polyn(x, p, degree=2, interaction_only=False, include_bias=True):\n",
    "    \"\"\"Multivariate polynomial - Numpy version\"\"\"\n",
    "    combs = [c for c in combinations(len(x), degree, interaction_only, include_bias)]\n",
    "    out = onp.array(p)\n",
    "    for (i, c) in enumerate(combs):\n",
    "        for j in c:\n",
    "            out[i] *= x[j]\n",
    "    return out.sum()\n",
    "\n",
    "@jax.jit\n",
    "def jpolyn(x, p, degree=2, interaction_only=False, include_bias=True):\n",
    "    \"\"\"Multivariate polynomial - JAX version\"\"\"\n",
    "    combs = [c for c in combinations(len(x), degree, interaction_only, include_bias)]\n",
    "    out = np.array(p)\n",
    "    for (i, c) in enumerate(combs):\n",
    "        for j in c:\n",
    "            out = jax.ops.index_update(out, jax.ops.index[i], out[i] * x[j])\n",
    "    return out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original      0.8804797755714371\n",
      "polyn (numpy) 0.8804796997954933\n",
      "polyn (jax)   0.8804792\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3], dtype=np.float32)\n",
    "print(\"original     \", model.predict(x.reshape(1, -1))[0])\n",
    "print(\"polyn (numpy)\", polyn(x, p))\n",
    "print(\"polyn (jax)  \", jpolyn(x, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have gradients for our polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 5.1778245, -1.7689846, -0.8483008], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(jpolyn, argnums=0)(x, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
